{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbb0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFake Audio Detection using MFCC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d533c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e588728",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d521331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genuine audio files: 26\n",
      "Number of deepfake audio files: 27\n",
      "\n",
      "Sample genuine files:\n",
      "- 2.wav\n",
      "- 21.wav\n",
      "- 22.wav\n",
      "\n",
      "Sample deepfake files:\n",
      "- 1.wav\n",
      "- 10.wav\n",
      "- 11.wav\n"
     ]
    }
   ],
   "source": [
    "# Set paths\n",
    "genuine_dir = \"real_audio\"\n",
    "deepfake_dir = \"deepfake_audio\"\n",
    "\n",
    "# List audio files\n",
    "genuine_files = glob.glob(os.path.join(genuine_dir, \"*.wav\"))\n",
    "deepfake_files = glob.glob(os.path.join(deepfake_dir, \"*.wav\"))\n",
    "\n",
    "print(f\"Number of genuine audio files: {len(genuine_files)}\")\n",
    "print(f\"Number of deepfake audio files: {len(deepfake_files)}\")\n",
    "\n",
    "# Display first few files from each directory\n",
    "print(\"\\nSample genuine files:\")\n",
    "for file in genuine_files[:3]:\n",
    "    print(f\"- {os.path.basename(file)}\")\n",
    "\n",
    "print(\"\\nSample deepfake files:\")\n",
    "for file in deepfake_files[:3]:\n",
    "    print(f\"- {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54197fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b1daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with file: 2.wav\n",
      "\n",
      "Feature shapes:\n",
      "MFCC shape: (13, 129)\n",
      "Spectral Contrast shape: (7, 129)\n",
      "Chroma shape: (12, 129)\n"
     ]
    }
   ],
   "source": [
    "# Test feature extraction on one file\n",
    "sample_file = genuine_files[0]\n",
    "print(f\"Testing with file: {os.path.basename(sample_file)}\")\n",
    "\n",
    "# Load audio\n",
    "audio_data, sr = librosa.load(sample_file, sr=None)\n",
    "\n",
    "# Extract features\n",
    "mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n",
    "spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr)\n",
    "chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)\n",
    "\n",
    "print(f\"\\nFeature shapes:\")\n",
    "print(f\"MFCC shape: {mfccs.shape}\")\n",
    "print(f\"Spectral Contrast shape: {spectral_contrast.shape}\")\n",
    "print(f\"Chroma shape: {chroma.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "167b8de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e6c909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (53, 32)\n",
      "Number of features: 32\n",
      "Class distribution: [26 27]\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists for features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Process genuine files\n",
    "for audio_path in genuine_files:\n",
    "    try:\n",
    "        # Load and extract features\n",
    "        audio_data, sr = librosa.load(audio_path, sr=None)\n",
    "        \n",
    "        # Extract features\n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)\n",
    "        \n",
    "        # Calculate means\n",
    "        mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "        spectral_contrast_mean = np.mean(spectral_contrast.T, axis=0)\n",
    "        chroma_mean = np.mean(chroma.T, axis=0)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = np.concatenate((mfccs_mean, spectral_contrast_mean, chroma_mean))\n",
    "        \n",
    "        X.append(combined_features)\n",
    "        y.append(0)  # 0 for genuine\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "# Process deepfake files\n",
    "for audio_path in deepfake_files:\n",
    "    try:\n",
    "        # Load and extract features\n",
    "        audio_data, sr = librosa.load(audio_path, sr=None)\n",
    "        \n",
    "        # Extract features\n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)\n",
    "        \n",
    "        # Calculate means\n",
    "        mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "        spectral_contrast_mean = np.mean(spectral_contrast.T, axis=0)\n",
    "        chroma_mean = np.mean(chroma.T, axis=0)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = np.concatenate((mfccs_mean, spectral_contrast_mean, chroma_mean))\n",
    "        \n",
    "        X.append(combined_features)\n",
    "        y.append(1)  # 1 for deepfake\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a67b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546eef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (42, 32)\n",
      "Testing set shape: (11, 32)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training set shape:\", X_train_scaled.shape)\n",
    "print(\"Testing set shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927c6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84672de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for linear kernel:\n",
      "Accuracy: 0.7273\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       0.71      0.83      0.77         6\n",
      "\n",
      "    accuracy                           0.73        11\n",
      "   macro avg       0.73      0.72      0.72        11\n",
      "weighted avg       0.73      0.73      0.72        11\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 2]\n",
      " [1 5]]\n",
      "\n",
      "Results for rbf kernel:\n",
      "Accuracy: 0.7273\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       0.71      0.83      0.77         6\n",
      "\n",
      "    accuracy                           0.73        11\n",
      "   macro avg       0.73      0.72      0.72        11\n",
      "weighted avg       0.73      0.73      0.72        11\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 2]\n",
      " [1 5]]\n",
      "\n",
      "Results for poly kernel:\n",
      "Accuracy: 0.8182\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       0.75      1.00      0.86         6\n",
      "\n",
      "    accuracy                           0.82        11\n",
      "   macro avg       0.88      0.80      0.80        11\n",
      "weighted avg       0.86      0.82      0.81        11\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 2]\n",
      " [0 6]]\n",
      "\n",
      "Best model: poly kernel with accuracy: 0.8182\n"
     ]
    }
   ],
   "source": [
    "# Train models with different kernels\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "results = {}\n",
    "\n",
    "for kernel in kernels:\n",
    "    # Train model\n",
    "    svm = SVC(kernel=kernel, random_state=42, probability=True)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[kernel] = {\n",
    "        'accuracy': accuracy,\n",
    "        'model': svm,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults for {kernel} kernel:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Get best model\n",
    "best_kernel = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "best_model = results[best_kernel]['model']\n",
    "\n",
    "print(f\"\\nBest model: {best_kernel} kernel with accuracy: {results[best_kernel]['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb4c68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a05bde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and scaler saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save model and scaler\n",
    "joblib.dump(best_model, \"enhanced_svm_model.pkl\")\n",
    "joblib.dump(scaler, \"enhanced_scaler.pkl\")\n",
    "print(\"Model and scaler saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd316ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db2ace25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The audio is classified as genuine with 96.40% confidence\n"
     ]
    }
   ],
   "source": [
    "def test_audio(audio_path):\n",
    "    # Load and process audio\n",
    "    audio_data, sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "    # Extract features\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr)\n",
    "    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)\n",
    "    \n",
    "    # Calculate means\n",
    "    mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "    spectral_contrast_mean = np.mean(spectral_contrast.T, axis=0)\n",
    "    chroma_mean = np.mean(chroma.T, axis=0)\n",
    "    \n",
    "    # Combine features\n",
    "    features = np.concatenate((mfccs_mean, spectral_contrast_mean, chroma_mean))\n",
    "    \n",
    "    # Scale features\n",
    "    features_scaled = scaler.transform(features.reshape(1, -1))\n",
    "    \n",
    "    # Predict\n",
    "    prediction = best_model.predict(features_scaled)\n",
    "    probability = best_model.predict_proba(features_scaled)[0]\n",
    "    \n",
    "    result = \"genuine\" if prediction[0] == 0 else \"deepfake\"\n",
    "    confidence = probability[prediction[0]] * 100\n",
    "    \n",
    "    return f\"The audio is classified as {result} with {confidence:.2f}% confidence\"\n",
    "\n",
    "# Test with a sample file\n",
    "test_file = \"real_audio/21.wav\"  # Replace with actual test file path\n",
    "print(test_audio(test_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b702eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
